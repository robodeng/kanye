{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Basics\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lyricsgenius\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pprint\n",
    "\n",
    "#NLP & ML\n",
    "import editdistance as ed\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1108)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "eng_stopwords = stopwords.words('english') \n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albums</th>\n",
       "      <th>track_number</th>\n",
       "      <th>track</th>\n",
       "      <th>writers</th>\n",
       "      <th>track_length</th>\n",
       "      <th>Year</th>\n",
       "      <th>old_kanye</th>\n",
       "      <th>clean_track</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The College Dropout</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro (Skit)</td>\n",
       "      <td>Kanye West</td>\n",
       "      <td>0:19</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro (Skit)</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n\\n[Intro: DeRay Davis]\\nKanye, can I talk to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The College Dropout</td>\n",
       "      <td>2</td>\n",
       "      <td>We Don't Care</td>\n",
       "      <td>West Miri Ben-Ari Ross Vannelli</td>\n",
       "      <td>3:59</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>We Don't Care</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n\\n[Intro]\\nOh yeah...\\nI got the perfect son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The College Dropout</td>\n",
       "      <td>3</td>\n",
       "      <td>Graduation Day</td>\n",
       "      <td>West John Stephens Ben-Ari</td>\n",
       "      <td>1:22</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduation Day</td>\n",
       "      <td>None</td>\n",
       "      <td>[Spoken: DeRay]\\nWhat in the fuck was that, Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The College Dropout</td>\n",
       "      <td>4</td>\n",
       "      <td>All Falls Down (featuring Syleena Johnson)</td>\n",
       "      <td>West Lauryn Hill</td>\n",
       "      <td>3:43</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>All Falls Down</td>\n",
       "      <td>Syleena_Johnson</td>\n",
       "      <td>[Chorus: Syleena Johnson &amp; Kanye West]\\nOh, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The College Dropout</td>\n",
       "      <td>5</td>\n",
       "      <td>I'll Fly Away</td>\n",
       "      <td>Albert E. Brumley</td>\n",
       "      <td>1:09</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>I'll Fly Away</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n\\nOne glad morning\\nWhen this life is over\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                albums  track_number  \\\n",
       "0  The College Dropout             1   \n",
       "1  The College Dropout             2   \n",
       "2  The College Dropout             3   \n",
       "3  The College Dropout             4   \n",
       "4  The College Dropout             5   \n",
       "\n",
       "                                        track  \\\n",
       "0                                Intro (Skit)   \n",
       "1                               We Don't Care   \n",
       "2                              Graduation Day   \n",
       "3  All Falls Down (featuring Syleena Johnson)   \n",
       "4                               I'll Fly Away   \n",
       "\n",
       "                           writers track_length  Year  old_kanye  \\\n",
       "0                       Kanye West         0:19  2004          1   \n",
       "1  West Miri Ben-Ari Ross Vannelli         3:59  2004          1   \n",
       "2       West John Stephens Ben-Ari         1:22  2004          1   \n",
       "3                 West Lauryn Hill         3:43  2004          1   \n",
       "4                Albert E. Brumley         1:09  2004          1   \n",
       "\n",
       "      clean_track         features  \\\n",
       "0    Intro (Skit)             None   \n",
       "1   We Don't Care             None   \n",
       "2  Graduation Day             None   \n",
       "3  All Falls Down  Syleena_Johnson   \n",
       "4   I'll Fly Away             None   \n",
       "\n",
       "                                              lyrics  \n",
       "0  \\n\\n[Intro: DeRay Davis]\\nKanye, can I talk to...  \n",
       "1  \\n\\n[Intro]\\nOh yeah...\\nI got the perfect son...  \n",
       "2  [Spoken: DeRay]\\nWhat in the fuck was that, Ka...  \n",
       "3  [Chorus: Syleena Johnson & Kanye West]\\nOh, wh...  \n",
       "4  \\n\\nOne glad morning\\nWhen this life is over\\n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye = pd.read_pickle(\"kanye_lyrics.pkl\")\n",
    "kanye_jik = pd.read_pickle('kanye_jik.pkl')\n",
    "kanye = pd.concat([kanye, kanye_jik])\n",
    "kanye.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The College Dropout',\n",
       " 'Late Registration',\n",
       " 'Graduation',\n",
       " '808s & Heartbreak',\n",
       " 'My Beautiful Dark Twisted Fantasy',\n",
       " 'Cruel Summer',\n",
       " 'Yeezus',\n",
       " 'The Life of Pablo',\n",
       " 'Ye',\n",
       " 'Singles',\n",
       " 'Cruel Winter',\n",
       " 'Jesus is King']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albums = list(kanye['albums'].unique())\n",
    "albums\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "\n",
    "Let's look at indirect/direct features for Kanye overall and individual albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indirect Features\n",
    "Some more experimental features.\n",
    "\n",
    " - count of sentences\n",
    "\n",
    " - count of words\n",
    "\n",
    " - count of unique words\n",
    "\n",
    " - count of letters\n",
    "\n",
    " - count of punctuations\n",
    "\n",
    " - count of uppercase words/letters\n",
    "\n",
    " - count of stop words\n",
    "\n",
    " - avg length of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indirect features\n",
    "\n",
    "#Sentense count in each comment:\n",
    "    #  '\\n' can be used to count the number of sentences in each comment\n",
    "kanye['count_sent']=kanye[\"lyrics\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "#Word count in each comment:\n",
    "kanye['count_word']=kanye[\"lyrics\"].apply(lambda x: len(str(x).split()))\n",
    "#Unique word count\n",
    "kanye['count_unique_word']=kanye[\"lyrics\"].apply(lambda x: len(set(str(x).split())))\n",
    "#Letter count\n",
    "kanye['count_letters']=kanye[\"lyrics\"].apply(lambda x: len(str(x)))\n",
    "#punctuation count\n",
    "kanye[\"count_punctuations\"] =kanye[\"lyrics\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#upper case words count\n",
    "kanye[\"count_words_upper\"] = kanye[\"lyrics\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#Number of stopwords\n",
    "kanye[\"count_stopwords\"] = kanye[\"lyrics\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "#Average length of the words\n",
    "kanye[\"mean_word_len\"] = kanye[\"lyrics\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "#derived features\n",
    "#Word count percent in each song:\n",
    "kanye['word_unique_percent'] = kanye['count_unique_word']*100/kanye['count_word']\n",
    "#Punct percent in each song:\n",
    "kanye['punct_percent']=kanye['count_punctuations']*100/kanye['count_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derived features\n",
    "#Word count percent in each song:\n",
    "kanye['word_unique_percent'] = kanye['count_unique_word']*100/kanye['count_word']\n",
    "#derived features\n",
    "#Punct percent in each song:\n",
    "kanye['punct_percent']=kanye['count_punctuations']*100/kanye['count_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_kanye</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>71.981132</td>\n",
       "      <td>452.433962</td>\n",
       "      <td>208.754717</td>\n",
       "      <td>2338.433962</td>\n",
       "      <td>120.603774</td>\n",
       "      <td>19.094340</td>\n",
       "      <td>183.924528</td>\n",
       "      <td>4.158140</td>\n",
       "      <td>47.206680</td>\n",
       "      <td>26.78931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>82.892857</td>\n",
       "      <td>579.285714</td>\n",
       "      <td>254.083333</td>\n",
       "      <td>2978.321429</td>\n",
       "      <td>148.011905</td>\n",
       "      <td>24.154762</td>\n",
       "      <td>239.416667</td>\n",
       "      <td>4.243931</td>\n",
       "      <td>47.280344</td>\n",
       "      <td>27.64002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_kanye  count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0          0   71.981132  452.433962         208.754717    2338.433962   \n",
       "1          1   82.892857  579.285714         254.083333    2978.321429   \n",
       "\n",
       "   count_punctuations  count_words_upper  count_stopwords  mean_word_len  \\\n",
       "0          120.603774          19.094340       183.924528       4.158140   \n",
       "1          148.011905          24.154762       239.416667       4.243931   \n",
       "\n",
       "   word_unique_percent  punct_percent  \n",
       "0            47.206680       26.78931  \n",
       "1            47.280344       27.64002  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[['old_kanye', 'count_sent', 'count_word', 'count_unique_word', \n",
    "       'count_letters', 'count_punctuations', 'count_words_upper',\n",
    "       'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent']].groupby(['old_kanye'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_kanye</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>452.433962</td>\n",
       "      <td>208.754717</td>\n",
       "      <td>183.924528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>579.285714</td>\n",
       "      <td>254.083333</td>\n",
       "      <td>239.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_kanye  count_word  count_unique_word  count_stopwords\n",
       "0          0  452.433962         208.754717       183.924528\n",
       "1          1  579.285714         254.083333       239.416667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[['old_kanye', 'count_word', 'count_unique_word', 'count_stopwords']].groupby(['old_kanye'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Old Kanye appeared to have been a lot more poetic back then and verbose with a more unique vocabulary and used a lot more grammar and punctuations. Further T-testing showed no significances for any of these features unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albums\n",
       "My Beautiful Dark Twisted Fantasy    95.692308\n",
       "Graduation                           91.692308\n",
       "The College Dropout                  83.904762\n",
       "Yeezus                               83.714286\n",
       "The Life of Pablo                    76.850000\n",
       "Singles                              73.142857\n",
       "Late Registration                    70.238095\n",
       "Ye                                   67.000000\n",
       "808s & Heartbreak                    61.166667\n",
       "Jesus is King                        49.181818\n",
       "Name: count_sent, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[~kanye['albums'].isin(['Cruel Winter', 'Cruel Summer'])].groupby(\n",
    "    'albums')['count_sent'].mean().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albums\n",
       "The College Dropout                  669.761905\n",
       "My Beautiful Dark Twisted Fantasy    661.076923\n",
       "Graduation                           562.692308\n",
       "Late Registration                    508.761905\n",
       "Yeezus                               497.714286\n",
       "The Life of Pablo                    486.400000\n",
       "Singles                              464.857143\n",
       "Ye                                   459.571429\n",
       "808s & Heartbreak                    361.000000\n",
       "Jesus is King                        291.272727\n",
       "Name: count_word, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[~kanye['albums'].isin(['Cruel Winter', 'Cruel Summer'])].groupby(\n",
    "    'albums')['count_word'].mean().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albums\n",
       "The College Dropout                  295.619048\n",
       "My Beautiful Dark Twisted Fantasy    289.538462\n",
       "Graduation                           238.076923\n",
       "Late Registration                    238.000000\n",
       "Yeezus                               237.428571\n",
       "The Life of Pablo                    223.500000\n",
       "Ye                                   220.000000\n",
       "Singles                              205.857143\n",
       "Jesus is King                        139.545455\n",
       "808s & Heartbreak                    134.916667\n",
       "Name: count_unique_word, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[~kanye['albums'].isin(['Cruel Winter', 'Cruel Summer'])].groupby(\n",
    "    'albums')['count_unique_word'].mean().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albums\n",
       "The College Dropout                  51.912524\n",
       "Late Registration                    49.922418\n",
       "My Beautiful Dark Twisted Fantasy    49.162116\n",
       "Jesus is King                        48.747641\n",
       "The Life of Pablo                    48.255908\n",
       "Ye                                   47.752725\n",
       "Yeezus                               47.406309\n",
       "Graduation                           43.357206\n",
       "Singles                              42.343336\n",
       "808s & Heartbreak                    37.111512\n",
       "Name: word_unique_percent, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye[~kanye['albums'].isin(['Cruel Winter', 'Cruel Summer'])].groupby(\n",
    "    'albums')['word_unique_percent'].mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def success_vs_fail_report(feature, try_log = False):\n",
    "    print (feature)\n",
    "    \n",
    "    success_values = kanye[feature][kanye['old_kanye'] == 1]\n",
    "    print (\"SUCCESS\", \"Mean:\", \"{:.3f}\".format(success_values.mean()), \"StdDev:\", \"{:.3f}\".format(success_values.std()))\n",
    "    \n",
    "    fail_values = kanye[feature][kanye['old_kanye'] == 0]\n",
    "    print (\"FAIL   \", \"Mean:\", \"{:.3f}\".format(fail_values.mean()), \"StdDev:\", \"{:.3f}\".format(fail_values.std()))\n",
    "    \n",
    "    ttest = ttest_ind(success_values, fail_values, equal_var=False)\n",
    "    print (\"\\nTStat:\", \"{:.3f}\".format(ttest.statistic))\n",
    "    print (\"P-Value\", \"{:.2f}\\n\".format(ttest.pvalue))\n",
    "    if try_log:\n",
    "        ttest_of_log = ttest_ind(np.log(success_values + 1), np.log(fail_values + 1), equal_var=False)\n",
    "        print (\"TStat (log):\", \"{:.3f}\".format(ttest_of_log.statistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_sent\n",
      "SUCCESS Mean: 82.893 StdDev: 41.096\n",
      "FAIL    Mean: 71.981 StdDev: 32.459\n",
      "\n",
      "TStat: 1.726\n",
      "P-Value 0.09\n",
      "\n",
      "count_word\n",
      "SUCCESS Mean: 579.286 StdDev: 389.445\n",
      "FAIL    Mean: 452.434 StdDev: 230.830\n",
      "\n",
      "TStat: 2.393\n",
      "P-Value 0.02\n",
      "\n",
      "count_unique_word\n",
      "SUCCESS Mean: 254.083 StdDev: 151.252\n",
      "FAIL    Mean: 208.755 StdDev: 108.480\n",
      "\n",
      "TStat: 2.039\n",
      "P-Value 0.04\n",
      "\n",
      "count_letters\n",
      "SUCCESS Mean: 2978.321 StdDev: 2007.068\n",
      "FAIL    Mean: 2338.434 StdDev: 1212.810\n",
      "\n",
      "TStat: 2.326\n",
      "P-Value 0.02\n",
      "\n",
      "count_punctuations\n",
      "SUCCESS Mean: 148.012 StdDev: 118.200\n",
      "FAIL    Mean: 120.604 StdDev: 61.775\n",
      "\n",
      "TStat: 1.775\n",
      "P-Value 0.08\n",
      "\n",
      "count_words_upper\n",
      "SUCCESS Mean: 24.155 StdDev: 23.227\n",
      "FAIL    Mean: 19.094 StdDev: 14.314\n",
      "\n",
      "TStat: 1.578\n",
      "P-Value 0.12\n",
      "\n",
      "count_stopwords\n",
      "SUCCESS Mean: 239.417 StdDev: 171.641\n",
      "FAIL    Mean: 183.925 StdDev: 98.494\n",
      "\n",
      "TStat: 2.402\n",
      "P-Value 0.02\n",
      "\n",
      "mean_word_len\n",
      "SUCCESS Mean: 4.244 StdDev: 1.098\n",
      "FAIL    Mean: 4.158 StdDev: 0.242\n",
      "\n",
      "TStat: 0.690\n",
      "P-Value 0.49\n",
      "\n",
      "word_unique_percent\n",
      "SUCCESS Mean: 47.280 StdDev: 12.337\n",
      "FAIL    Mean: 47.207 StdDev: 9.871\n",
      "\n",
      "TStat: 0.039\n",
      "P-Value 0.97\n",
      "\n",
      "punct_percent\n",
      "SUCCESS Mean: 27.640 StdDev: 20.527\n",
      "FAIL    Mean: 26.789 StdDev: 7.915\n",
      "\n",
      "TStat: 0.342\n",
      "P-Value 0.73\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indirect_features = ['count_sent', 'count_word', 'count_unique_word', \n",
    "       'count_letters', 'count_punctuations', 'count_words_upper',\n",
    "       'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent']\n",
    "[success_vs_fail_report(i) for i in indirect_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at some of the most common words and phrases Kanye uses. Minus the obscenity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The College Dropout \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 114),\n",
      "    (('and', 'i'), 77),\n",
      "    (('don', 't'), 69),\n",
      "    (('ain', 't'), 68),\n",
      "    (('it', 's'), 52),\n",
      "    (('you', 'know'), 49),\n",
      "    (('to', 'the'), 46),\n",
      "    (('that', 's'), 44),\n",
      "    (('in', 'the'), 44),\n",
      "    (('kanye', 'west'), 42)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('are', 'you', 'gonna'), 30),\n",
      "    (('you', 'gonna', 'be'), 30),\n",
      "    (('i', 'ain', 't'), 28),\n",
      "    (('all', 'these', 'things'), 26),\n",
      "    (('gonna', 'be', 'are'), 24),\n",
      "    (('be', 'are', 'you'), 24),\n",
      "    (('get', 'up', 'i'), 23),\n",
      "    (('up', 'i', 'get'), 23),\n",
      "    (('things', 'all', 'these'), 22),\n",
      "    (('these', 'things', 'all'), 20)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('are', 'you', 'gonna', 'be'), 30),\n",
      "    (('you', 'gonna', 'be', 'are'), 24),\n",
      "    (('gonna', 'be', 'are', 'you'), 24),\n",
      "    (('be', 'are', 'you', 'gonna'), 24),\n",
      "    (('get', 'up', 'i', 'get'), 23),\n",
      "    (('all', 'these', 'things', 'all'), 20),\n",
      "    (('these', 'things', 'all', 'these'), 20),\n",
      "    (('things', 'all', 'these', 'things'), 20),\n",
      "    (('it', 'all', 'falls', 'down'), 19),\n",
      "    (('all', 'it', 'all', 'falls'), 18)]\n",
      "\n",
      "\n",
      "Late Registration \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 102),\n",
      "    (('kanye', 'west'), 75),\n",
      "    (('ain', 't'), 56),\n",
      "    (('can', 't'), 44),\n",
      "    (('it', 's'), 40),\n",
      "    (('that', 's'), 40),\n",
      "    (('in', 'the'), 39),\n",
      "    (('don', 't'), 35),\n",
      "    (('and', 'i'), 31),\n",
      "    (('get', 'down'), 28)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('i', 'can', 't'), 19),\n",
      "    (('hook', 'kanye', 'west'), 17),\n",
      "    (('can', 't', 'wait'), 17),\n",
      "    (('ain', 't', 'got'), 16),\n",
      "    (('i', 'ain', 't'), 16),\n",
      "    (('i', 'gotta', 'leave'), 16),\n",
      "    (('gotta', 'leave', 'get'), 16),\n",
      "    (('leave', 'get', 'down'), 16),\n",
      "    (('get', 'down', 'girl'), 16),\n",
      "    (('down', 'girl', 'go'), 16)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('i', 'gotta', 'leave', 'get'), 16),\n",
      "    (('gotta', 'leave', 'get', 'down'), 16),\n",
      "    (('leave', 'get', 'down', 'girl'), 16),\n",
      "    (('get', 'down', 'girl', 'go'), 16),\n",
      "    (('down', 'girl', 'go', 'head'), 16),\n",
      "    (('la', 'lah', 'la', 'la'), 13),\n",
      "    (('lah', 'la', 'la', 'la'), 13),\n",
      "    (('c', 'mon', 'homie', 'we'), 13),\n",
      "    (('mon', 'homie', 'we', 'major'), 13),\n",
      "    (('d', 'by', 'kanye', 'west'), 12)]\n",
      "\n",
      "\n",
      "Graduation \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 76),\n",
      "    (('can', 't'), 58),\n",
      "    (('kanye', 'west'), 53),\n",
      "    (('i', 'can'), 42),\n",
      "    (('don', 't'), 41),\n",
      "    (('in', 'the'), 32),\n",
      "    (('yeah', 'my'), 30),\n",
      "    (('t', 'study'), 28),\n",
      "    (('if', 'you'), 26),\n",
      "    (('told', 'ya'), 25)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('i', 'can', 't'), 40),\n",
      "    (('can', 't', 'study'), 28),\n",
      "    (('drunk', 'and', 'hot'), 21),\n",
      "    (('t', 'study', 'no'), 20),\n",
      "    (('study', 'no', 'yeah'), 20),\n",
      "    (('no', 'yeah', 'my'), 20),\n",
      "    (('yeah', 'my', 'i'), 19),\n",
      "    (('my', 'i', 'can'), 18),\n",
      "    (('you', 'can', 't'), 16),\n",
      "    (('the', 'good', 'life'), 15)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('i', 'can', 't', 'study'), 28),\n",
      "    (('can', 't', 'study', 'no'), 20),\n",
      "    (('t', 'study', 'no', 'yeah'), 20),\n",
      "    (('study', 'no', 'yeah', 'my'), 20),\n",
      "    (('yeah', 'my', 'i', 'can'), 18),\n",
      "    (('my', 'i', 'can', 't'), 18),\n",
      "    (('you', 'can', 't', 'tell'), 14),\n",
      "    (('told', 'ya', 'told', 'ya'), 14),\n",
      "    (('drunk', 'and', 'hot', 'girls'), 14),\n",
      "    (('can', 't', 'tell', 'me'), 13)]\n",
      "\n",
      "\n",
      "808s & Heartbreak \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 54),\n",
      "    (('don', 't'), 46),\n",
      "    (('so', 'amazing'), 36),\n",
      "    (('amazing', 'so'), 35),\n",
      "    (('kanye', 'west'), 31),\n",
      "    (('can', 't'), 28),\n",
      "    (('it', 's'), 26),\n",
      "    (('in', 'the'), 25),\n",
      "    (('you', 'know'), 25),\n",
      "    (('you', 'will'), 24)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('your', 'love', 'lockdown'), 21),\n",
      "    (('that', 'you', 'know'), 21),\n",
      "    (('i', 'can', 't'), 17),\n",
      "    (('it', 's', 'amazing'), 16),\n",
      "    (('the', 'wrong', 'things'), 16),\n",
      "    (('can', 't', 'stop'), 15),\n",
      "    (('don', 't', 'know'), 15),\n",
      "    (('they', 'don', 't'), 13),\n",
      "    (('so', 'amazing', 'it'), 12),\n",
      "    (('amazing', 'it', 's'), 12)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('so', 'amazing', 'so', 'amazing'), 24),\n",
      "    (('amazing', 'so', 'amazing', 'so'), 23),\n",
      "    (('they', 'don', 't', 'know'), 12),\n",
      "    (('amazing', 'so', 'amazing', 'it'), 12),\n",
      "    (('so', 'amazing', 'it', 's'), 12),\n",
      "    (('amazing', 'it', 's', 'amazing'), 12),\n",
      "    (('it', 's', 'amazing', 'so'), 11),\n",
      "    (('s', 'amazing', 'so', 'amazing'), 11),\n",
      "    (('i', 'can', 't', 'stop'), 10),\n",
      "    (('how', 'could', 'you', 'be'), 10)]\n",
      "\n",
      "\n",
      "My Beautiful Dark Twisted Fantasy \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 114),\n",
      "    (('kanye', 'west'), 63),\n",
      "    (('in', 'the'), 49),\n",
      "    (('the', 'lights'), 41),\n",
      "    (('for', 'the'), 39),\n",
      "    (('look', 'at'), 35),\n",
      "    (('don', 't'), 33),\n",
      "    (('let', 's'), 32),\n",
      "    (('it', 's'), 31),\n",
      "    (('and', 'i'), 30)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('look', 'at', 'ya'), 27),\n",
      "    (('at', 'ya', 'look'), 22),\n",
      "    (('ya', 'look', 'at'), 22),\n",
      "    (('for', 'the', 'night'), 19),\n",
      "    (('all', 'of', 'the'), 18),\n",
      "    (('of', 'the', 'lights'), 18),\n",
      "    (('down', 'for', 'the'), 18),\n",
      "    (('you', 're', 'my'), 16),\n",
      "    (('have', 'a', 'toast'), 15),\n",
      "    (('don', 't', 'really'), 13)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('look', 'at', 'ya', 'look'), 22),\n",
      "    (('at', 'ya', 'look', 'at'), 22),\n",
      "    (('ya', 'look', 'at', 'ya'), 22),\n",
      "    (('all', 'of', 'the', 'lights'), 18),\n",
      "    (('down', 'for', 'the', 'night'), 18),\n",
      "    (('if', 'you', 'don', 't'), 13),\n",
      "    (('i', 'ma', 'need', 'to'), 12),\n",
      "    (('ma', 'need', 'to', 'see'), 12),\n",
      "    (('need', 'to', 'see', 'your'), 12),\n",
      "    (('to', 'see', 'your', 'fucking'), 12)]\n",
      "\n",
      "\n",
      "Cruel Summer \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 48),\n",
      "    (('that', 'body'), 30),\n",
      "    (('and', 'a'), 23),\n",
      "    (('shake', 'that'), 22),\n",
      "    (('g', 'o'), 21),\n",
      "    (('o', 'd'), 21),\n",
      "    (('body', 'party'), 21),\n",
      "    (('party', 'that'), 21),\n",
      "    (('kanye', 'west'), 18),\n",
      "    (('that', 's'), 17)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('shake', 'that', 'body'), 22),\n",
      "    (('that', 'body', 'party'), 21),\n",
      "    (('body', 'party', 'that'), 21),\n",
      "    (('it', 'is', 'a'), 16),\n",
      "    (('party', 'that', 'bod'), 14),\n",
      "    (('o', 'd', 'music'), 13),\n",
      "    (('i', 'm', 'in'), 12),\n",
      "    (('they', 'want', 'the'), 12),\n",
      "    (('is', 'a', 'weepin'), 11),\n",
      "    (('a', 'weepin', 'and'), 11)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('shake', 'that', 'body', 'party'), 21),\n",
      "    (('that', 'body', 'party', 'that'), 21),\n",
      "    (('body', 'party', 'that', 'bod'), 14),\n",
      "    (('g', 'o', 'o', 'd'), 13),\n",
      "    (('it', 'is', 'a', 'weepin'), 11),\n",
      "    (('is', 'a', 'weepin', 'and'), 11),\n",
      "    (('a', 'weepin', 'and', 'a'), 11),\n",
      "    (('and', 'a', 'gnashin', 'of'), 11),\n",
      "    (('a', 'gnashin', 'of', 'teeth'), 11),\n",
      "    (('o', 'okay', 'lamborghini', 'mercy'), 10)]\n",
      "\n",
      "\n",
      "Yeezus \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 38),\n",
      "    (('dem', 'a'), 30),\n",
      "    (('can', 't'), 28),\n",
      "    (('kanye', 'west'), 27),\n",
      "    (('on', 'the'), 18),\n",
      "    (('a', 'gwaan'), 18),\n",
      "    (('that', 's'), 17),\n",
      "    (('it', 's'), 16),\n",
      "    (('i', 'can'), 15),\n",
      "    (('don', 't'), 14)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('dem', 'a', 'gwaan'), 18),\n",
      "    (('a', 'gwaan', 'dem'), 12),\n",
      "    (('gwaan', 'dem', 'a'), 12),\n",
      "    (('we', 'can', 'send'), 12),\n",
      "    (('can', 'send', 'this'), 12),\n",
      "    (('send', 'this', 'bitch'), 12),\n",
      "    (('this', 'bitch', 'up'), 12),\n",
      "    (('bitch', 'up', 'it'), 12),\n",
      "    (('up', 'it', 'can'), 12),\n",
      "    (('it', 'can', 'go'), 12)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('dem', 'a', 'gwaan', 'dem'), 12),\n",
      "    (('a', 'gwaan', 'dem', 'a'), 12),\n",
      "    (('dem', 'a', 'dem', 'a'), 12),\n",
      "    (('we', 'can', 'send', 'this'), 12),\n",
      "    (('can', 'send', 'this', 'bitch'), 12),\n",
      "    (('send', 'this', 'bitch', 'up'), 12),\n",
      "    (('this', 'bitch', 'up', 'it'), 12),\n",
      "    (('bitch', 'up', 'it', 'can'), 12),\n",
      "    (('up', 'it', 'can', 'go'), 12),\n",
      "    (('it', 'can', 'go', 'down'), 12)]\n",
      "\n",
      "\n",
      "The Life of Pablo \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('i', 'm'), 98),\n",
      "    (('don', 't'), 72),\n",
      "    (('kanye', 'west'), 62),\n",
      "    (('in', 'the'), 55),\n",
      "    (('i', 'feel'), 44),\n",
      "    (('it', 's'), 44),\n",
      "    (('feel', 'it'), 36),\n",
      "    (('ain', 't'), 33),\n",
      "    (('i', 'know'), 26),\n",
      "    (('you', 're'), 26)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('i', 'feel', 'it'), 34),\n",
      "    (('i', 'don', 't'), 21),\n",
      "    (('the', 'night', 'sky'), 17),\n",
      "    (('in', 'the', 'night'), 16),\n",
      "    (('no', 'more', 'parties'), 12),\n",
      "    (('more', 'parties', 'in'), 12),\n",
      "    (('parties', 'in', 'l'), 12),\n",
      "    (('in', 'l', 'a'), 12),\n",
      "    (('i', 've', 'been'), 11),\n",
      "    (('you', 're', 'lookin'), 11)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('bam', 'ey', 'ey', 'ey'), 16),\n",
      "    (('in', 'the', 'night', 'sky'), 16),\n",
      "    (('no', 'more', 'parties', 'in'), 12),\n",
      "    (('more', 'parties', 'in', 'l'), 12),\n",
      "    (('parties', 'in', 'l', 'a'), 12),\n",
      "    (('you', 're', 'lookin', 'at'), 11),\n",
      "    (('re', 'lookin', 'at', 'the'), 11),\n",
      "    (('lookin', 'at', 'the', 'church'), 11),\n",
      "    (('at', 'the', 'church', 'in'), 11),\n",
      "    (('the', 'church', 'in', 'the'), 11)]\n",
      "\n",
      "\n",
      "Ye \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('don', 't'), 30),\n",
      "    (('i', 'know'), 21),\n",
      "    (('i', 'm'), 20),\n",
      "    (('and', 'i'), 17),\n",
      "    (('kanye', 'west'), 16),\n",
      "    (('love', 'you'), 14),\n",
      "    (('that', 's'), 14),\n",
      "    (('i', 'don'), 14),\n",
      "    (('i', 'ma'), 12),\n",
      "    (('on', 'my'), 11)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('i', 'don', 't'), 14),\n",
      "    (('make', 'no', 'mistake'), 10),\n",
      "    (('no', 'mistake', 'girl'), 10),\n",
      "    (('mistake', 'girl', 'still'), 10),\n",
      "    (('girl', 'still', 'love'), 10),\n",
      "    (('still', 'love', 'you'), 10),\n",
      "    (('believe', 'it', 'or'), 8),\n",
      "    (('it', 'or', 'not'), 8),\n",
      "    (('i', 'know', 'you'), 7),\n",
      "    (('you', 'make', 'no'), 7)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('make', 'no', 'mistake', 'girl'), 10),\n",
      "    (('no', 'mistake', 'girl', 'still'), 10),\n",
      "    (('mistake', 'girl', 'still', 'love'), 10),\n",
      "    (('girl', 'still', 'love', 'you'), 10),\n",
      "    (('believe', 'it', 'or', 'not'), 8),\n",
      "    (('i', 'know', 'i', 'know'), 6),\n",
      "    (('shit', 'could', 'get', 'menacin'), 6),\n",
      "    (('could', 'get', 'menacin', 'frightenin'), 6),\n",
      "    (('get', 'menacin', 'frightenin', 'find'), 6),\n",
      "    (('menacin', 'frightenin', 'find', 'help'), 6)]\n",
      "\n",
      "\n",
      "Singles \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('all', 'day'), 24),\n",
      "    (('day', 'nigga'), 23),\n",
      "    (('you', 're'), 21),\n",
      "    (('i', 'm'), 21),\n",
      "    (('on', 'your'), 19),\n",
      "    (('kanye', 'west'), 19),\n",
      "    (('lift', 'yourself'), 19),\n",
      "    (('up', 'on'), 19),\n",
      "    (('yourself', 'up'), 18),\n",
      "    (('your', 'feet'), 18)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('all', 'day', 'nigga'), 22),\n",
      "    (('lift', 'yourself', 'up'), 18),\n",
      "    (('yourself', 'up', 'on'), 18),\n",
      "    (('up', 'on', 'your'), 18),\n",
      "    (('on', 'your', 'feet'), 18),\n",
      "    (('i', 'love', 'it'), 13),\n",
      "    (('your', 'feet', 'let'), 12),\n",
      "    (('feet', 'let', 's'), 12),\n",
      "    (('let', 's', 'get'), 12),\n",
      "    (('s', 'get', 'it'), 12)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('lift', 'yourself', 'up', 'on'), 18),\n",
      "    (('yourself', 'up', 'on', 'your'), 18),\n",
      "    (('up', 'on', 'your', 'feet'), 18),\n",
      "    (('on', 'your', 'feet', 'let'), 12),\n",
      "    (('your', 'feet', 'let', 's'), 12),\n",
      "    (('feet', 'let', 's', 'get'), 12),\n",
      "    (('let', 's', 'get', 'it'), 12),\n",
      "    (('s', 'get', 'it', 'on'), 12),\n",
      "    (('get', 'it', 'on', 'lift'), 11),\n",
      "    (('it', 'on', 'lift', 'yourself'), 11)]\n",
      "\n",
      "\n",
      "Cruel Winter \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('round', 'and'), 26),\n",
      "    (('and', 'round'), 26),\n",
      "    (('go', 'round'), 18),\n",
      "    (('i', 'm'), 16),\n",
      "    (('round', 'they'), 16),\n",
      "    (('they', 'go'), 16),\n",
      "    (('round', 'it'), 9),\n",
      "    (('it', 'go'), 9),\n",
      "    (('don', 't'), 7),\n",
      "    (('all', 'in'), 6)]\n",
      "\n",
      "Trigrams\n",
      "\n",
      "[   (('go', 'round', 'and'), 18),\n",
      "    (('and', 'round', 'they'), 16),\n",
      "    (('round', 'they', 'go'), 16),\n",
      "    (('and', 'round', 'it'), 9),\n",
      "    (('round', 'it', 'go'), 9),\n",
      "    (('it', 'go', 'round'), 9),\n",
      "    (('they', 'go', 'round'), 8),\n",
      "    (('all', 'in', 'singles'), 5),\n",
      "    (('in', 'singles', 'straight'), 5),\n",
      "    (('singles', 'straight', 'up'), 5)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('go', 'round', 'and', 'round'), 18),\n",
      "    (('round', 'and', 'round', 'they'), 16),\n",
      "    (('and', 'round', 'they', 'go'), 16),\n",
      "    (('round', 'and', 'round', 'it'), 9),\n",
      "    (('and', 'round', 'it', 'go'), 9),\n",
      "    (('round', 'it', 'go', 'round'), 9),\n",
      "    (('it', 'go', 'round', 'and'), 9),\n",
      "    (('round', 'they', 'go', 'round'), 8),\n",
      "    (('they', 'go', 'round', 'and'), 8),\n",
      "    (('all', 'in', 'singles', 'straight'), 5)]\n",
      "\n",
      "\n",
      "Jesus is King \n",
      "\n",
      "\n",
      "Bigrams\n",
      "\n",
      "[   (('we', 'need'), 30),\n",
      "    (('of', 'the'), 26),\n",
      "    (('the', 'lord'), 25),\n",
      "    (('til', 'the'), 23),\n",
      "    (('the', 'power'), 22),\n",
      "    (('i', 'm'), 21),\n",
      "    (('that', 's'), 20),\n",
      "    (('sing', 'til'), 19),\n",
      "    (('need', 'you'), 18),\n",
      "    (('power', 'of'), 16)]\n",
      "\n",
      "Trigrams\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (('of', 'the', 'lord'), 23),\n",
      "    (('til', 'the', 'power'), 22),\n",
      "    (('sing', 'til', 'the'), 19),\n",
      "    (('we', 'need', 'you'), 18),\n",
      "    (('the', 'power', 'of'), 16),\n",
      "    (('power', 'of', 'the'), 16),\n",
      "    (('the', 'lord', 'comes'), 15),\n",
      "    (('lord', 'comes', 'down'), 15),\n",
      "    (('that', 's', 'on'), 12),\n",
      "    (('we', 'have', 'everything'), 10)]\n",
      "\n",
      "Quadgrams\n",
      "\n",
      "[   (('sing', 'til', 'the', 'power'), 19),\n",
      "    (('til', 'the', 'power', 'of'), 16),\n",
      "    (('the', 'power', 'of', 'the'), 16),\n",
      "    (('power', 'of', 'the', 'lord'), 16),\n",
      "    (('of', 'the', 'lord', 'comes'), 15),\n",
      "    (('the', 'lord', 'comes', 'down'), 15),\n",
      "    (('we', 'have', 'everything', 'we'), 10),\n",
      "    (('have', 'everything', 'we', 'need'), 10),\n",
      "    (('down', 'sing', 'til', 'the'), 9),\n",
      "    (('everything', 'we', 'need', 'ooh'), 8)]\n"
     ]
    }
   ],
   "source": [
    "#For each album, what are some of the top bi, tri, and quadgrams as Kanye's hallmark phrases\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "\n",
    "def albumify(df, n):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    \n",
    "    for i in list(df['albums'].unique()):\n",
    "        album_lyrics = df[df['albums'] == i]['lyrics'].str.lower().apply(\n",
    "            lambda x: re.sub(r'intro', r'', x)).apply(\n",
    "            lambda x: re.sub(r'\\d+', r' ', x)).apply(\n",
    "            lambda y: re.sub(r'\\W+', r' ',y)).apply(\n",
    "            lambda z: re.sub(r\"_+\",r\" \", z)).apply(\n",
    "            lambda y: re.sub(r'verse', r'', y)).apply(\n",
    "            lambda z: re.sub(r'chorus', r'', z)).apply(\n",
    "            lambda a: re.sub(r'produce', r'', a)).apply(\n",
    "            lambda x: re.sub(\"[\\[].*?[\\]]\", \"\", x)).str.split().apply(\n",
    "            lambda x: ' '.join(x)).apply(\n",
    "            lambda u: re.sub(r'   ', r'', u)).apply(\n",
    "            lambda v: re.sub(r'  ', r'', v))\n",
    "\n",
    "        print('\\n')\n",
    "        print(i, '\\n')\n",
    "        bigram = album_lyrics.map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
    "        bigrams = bigram.tolist()\n",
    "        bigrams = list(chain(*bigrams))\n",
    "        bigrams = [(x.lower(), y.lower()) for x,y in bigrams if x != y]\n",
    "        bigrams_counts = Counter(bigrams)\n",
    "        print('\\nBigrams\\n')\n",
    "        pp.pprint(bigrams_counts.most_common(n))\n",
    "        \n",
    "        trigram = album_lyrics.map(lambda x: find_ngrams(x.split(\" \"), 3))\n",
    "        trigrams = trigram.tolist()\n",
    "        trigrams = list(chain(*trigrams))\n",
    "        trigrams = [(x.lower(), y.lower(), z.lower()) for x, y, z in trigrams if (x != y and y != z and x != z) and (x != z)]\n",
    "        trigrams_counts = Counter(trigrams)\n",
    "        print('\\nTrigrams\\n')\n",
    "        pp.pprint(trigrams_counts.most_common(n))\n",
    "        \n",
    "        quadgram = album_lyrics.map(lambda x: find_ngrams(x.split(\" \"), 4))\n",
    "        quadgrams = quadgram.tolist()\n",
    "        quadgrams = list(chain(*quadgrams))\n",
    "        quadgrams = [(x.lower(), y.lower(), z.lower(), a.lower()) for x, y, z, a in quadgrams if x != y]\n",
    "        quadgrams_counts = Counter(quadgrams)\n",
    "        print('\\nQuadgrams\\n')        \n",
    "        pp.pprint(quadgrams_counts.most_common(n))\n",
    "        \n",
    "\n",
    "albumify(kanye, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "def nltk_preprocess(data):\n",
    "    '''This function preprocesses a data frame, specifing a text_column, \n",
    "    and strips down the document to cleaned, individualized word tokens without\n",
    "    stop words and other excessive parts of speech and eventually rejoins the remaining words.\n",
    "    '''\n",
    "    #Initializes stop words and new column creation\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize Lemmatizer object and final list of lemmatized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None # for easy if-statement\n",
    "        \n",
    "    def lemmatized(word, tag):\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma = str(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            lemma = str(lemmatizer.lemmatize(word, pos=wntag))\n",
    "        return lemma\n",
    "\n",
    "    #Song Specifics\n",
    "    data = data.str.lower()\n",
    "    data = data.apply(lambda x: re.sub(r'intro', r'', x)).apply(\n",
    "        lambda y: re.sub(r'verse', r'', y)).apply(\n",
    "        lambda z: re.sub(r'chorus', r'', z)).apply(\n",
    "        lambda a: re.sub(r'produce', r'', a))    \n",
    "    data = data.apply(lambda x: re.sub(\"[\\[].*?[\\]]\", \"\", x))\n",
    "\n",
    "    #Basic\n",
    "    data = data.apply(\n",
    "        lambda x: re.sub(r'\\d+', r' ', x)).apply(\n",
    "        lambda y: re.sub(r'\\W+', r' ', y)).apply(\n",
    "        lambda z: re.sub(r\"_+\",r\" \",z))\n",
    "    data = data.apply(word_tokenize)\n",
    "    data = data.apply(lambda x: [item for item in x if item not in stop])\n",
    "    data = data.apply(pos_tag)\n",
    "    data = data.apply(lambda x: [lemmatized(word, tag) for (word, tag) in x])\n",
    "    data = data.apply(lambda x: ' '.join(x))\n",
    "    data = data.apply(lambda u: re.sub(r'   ', r'', u)).apply(lambda v: re.sub(r'  ', r'', v))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kanye['processed_lyrics'] = nltk_preprocess(kanye['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "cv = vectorizer.fit_transform(kanye['processed_lyrics'])\n",
    "cv_freq = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences':np.asarray(cv.sum(axis=0)).ravel().tolist()})\n",
    "cv_freq['frequency'] = cv_freq['occurrences']/cv_freq.shape[0]\n",
    "cv_freq.sort_values('frequency', ascending = False).head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanye.to_pickle('kanye_modeling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
